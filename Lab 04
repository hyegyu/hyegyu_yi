#Hypothesis using matrix
#data and lebel   #입력값:X1,X2,X3    결과값:Y
x1 = [ 73.,  93.,  89.,  96.,  73.]
x2 = [ 80.,  88.,  91.,  98.,  66.]
x3 = [ 75.,  93.,  90., 100.,  70.]
Y  = [152., 185., 180., 196., 142.]                    #데이터의 입력

# weights
w1 = tf.Variable(10.)
w2 = tf.Variable(10.)
w3 = tf.Variable(10.)
b  = tf.Variable(10.)

hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b


#전체실행코드
# data and label  #입력값:X1,X2,X3    결과값:Y
x1 = [ 73.,  93.,  89.,  96.,  73.]
x2 = [ 80.,  88.,  91.,  98.,  66.]
x3 = [ 75.,  93.,  90., 100.,  70.]
Y  = [152., 185., 180., 196., 142.]

# random weights                  #초기값은 전부 1 (변수값대로 W의 값을 지정해야 함)
w1 = tf.Variable(tf.random_normal([1]))
w2 = tf.Variable(tf.random_normal([1]))
w3 = tf.Variable(tf.random_normal([1]))
b  = tf.Variable(tf.random_normal([1]))

learning_rate = 0.000001

for i in range(1000+1):            #1001번 반복
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:           #Gradient descent를 이용. GradientTape를 이용하여 tape에 기록
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b                 #Hypothesis 함수
        cost = tf.reduce_mean(tf.square(hypothesis - Y))              #cost함수(Hypothesis와 결과값의 편차 제곱의 평균값)
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])           # tape를 Gradient로 불러서 4개의 변수에 대한 기울기 값을 구함
 
    # update w1,w2,w3 and b         #assign_sub을 이용하여 각각에 할당(꼭 다 따로 써야함.)
    w1.assign_sub(learning_rate * w1_grad)
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:                  #50번에 한번씩 cost값을 나타내서 변화를 봄 
      print("{:5} | {:12.4f}".format(i, cost.numpy()))

#결과값 : cost값이 줄어들다가 일정 기간부터는 천천히 줄어듦
    i          cost
    0 |   11325.9121
   50 |     135.3618
  100 |      11.1817
  150 |       9.7940
  200 |       9.7687
  250 |       9.7587
  300 |       9.7489
  350 |       9.7389
  400 |       9.7292
  450 |       9.7194
  500 |       9.7096
  550 |       9.6999
  600 |       9.6903
  650 |       9.6806
  700 |       9.6709
  750 |       9.6612
  800 |       9.6517
  850 |       9.6421
  900 |       9.6325
  950 |       9.6229
 1000 |       9.6134


#Matrix
data = np.array([
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)                              #데이터 입력

# slice data
X = data[:, :-1]              #X=입력값. 5행 3열까지 지정
y = data[:, [-1]]             #Y=결과. 5행 4열만 지정

W = tf.Variable(tf.random_normal([3, 1]))       
b = tf.Variable(tf.random_normal([1]))          

learning_rate = 0.000001      #작은 수로 지정

# hypothesis, prediction function
def predict(X):                                 #예측함수 정의
    return tf.matmul(X, W) + b                  

n_epochs = 2000                 #2000번 반복
for i in range(n_epochs+1):     #총 2001번 반복
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))       #비용함수의 경사도를 기록

    W_grad, b_grad = tape.gradient(cost, [W, b])           #손실의 경사도를 구함

    W.assign_sub(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad)                   # W와 b값을 지속적으로 입력 
    if i % 100 == 0:
        print("{:5} | {:10.4f}".format(i, cost.numpy()))   #100번에 한번씩 나오게해서 값의 변화를 봄
          
#결과값 : 처음의 cost값은 크지만 급속히 줄어서 더이상 줄어들지 않아진다.(최적화가 급격히 진행됨)
 epoch      cost
    0 |  5455.5903
  100 |    31.7443
  200 |    30.9326
  300 |    30.7894
  400 |    30.6468
  500 |    30.5055
  600 |    30.3644
  700 |    30.2242
  800 |    30.0849
  900 |    29.9463
 1000 |    29.8081
 1100 |    29.6710
 1200 |    29.5348
 1300 |    29.3989
 1400 |    29.2641
 1500 |    29.1299
 1600 |    28.9961
 1700 |    28.8634
 1800 |    28.7313
 1900 |    28.5997
 2000 |    28.4689


#원래는 각각 하나하나 따로 지정해야하지만 Matrix연산을 사용하여 각 항목을 1줄로 표햔할 수 있다.
W = tf.Variable(tf.random_normal([3, 1]))             #25~27번째 줄을 요약
tf.matmul(X, W) + b                                   #14번째 줄을 요약
  W.assign_sub(learning_rate * W_grad)                #41~43번째 줄을 요약
