#Logistic Regression_code   #음의 선형관계
x_train = [[1, 2],          #학습을 위한 x데이터
          [2, 3],
          [3, 1],
          [4, 3],
          [5, 3],
          [6, 2]]
y_train = [[0],             #학습을 위한 y데이터
          [0],
          [0],
          [1],
          [1],
          [1]]

x_test = [[5,2]]
y_test = [[1]]

import tensorflow.contrib.eager as tfe        #텐서플로우에서 eager모드로 실행하기위해 선언
tf.enable_execution()                         #eager모드로 선언
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))    #tf데이터를 통해 x값과 y값을 x의 길이(6)만큼 학습
W = tf.Variable(tf.zeros([2,1]), name='weight')
b = tf.Variable(tf.zeros([1]), name='bias')         #모델 선언

def logistic_regression(features):
    hypothesis  = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b))         #가셜= logistic regressio을 하기 위해 나온 값을 시그모이드 함수를 이용
    return hypothesis                               # logistic regression
    
def loss_fn(hypothesis, features, labels):          #cost를 구하기 위함
    cost = -tf.reduce_mean(labels * tf.log(loss_fn(hypothesis))) + (1 - labels) * tf.log(1 - hypothesis))      #lable과 fearure의 값을 이용하여 cost값 구현
    return cost       #cost값 구현

def grad(hypothesis, features, labels):             #학습을 위한 함수
    with tf.GradientTape() as tape:
        loss_value = loss_fn(hypothesis, labels)        #실제값과 가설의 값을 비교한 loss값을 구함
    return tape.gradient(loss_value, [W,b])             #gradient를 통해 실제 모델값 변경
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)       #최적화 선언

for step in range(EPOCHS):                          #기존의 텐서플로우는 그래프를 그리고 run하지만 eager모드에서는 나중에 그래프를 그림
    for features, labels  in tfe.Iterator(dataset):       #위의 데이터를 기초로 Iterator를 돌리며 x와 y값을 넣으며 모델 생성
        grads = grad(logistic_regression(features), features, labels)     #가설을 통해 학습을 위한 함수
        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))        #최소화
        if step % 100 == 0:                                     #100번마다 값을 나오게 함
            print("Iter: {}, Loss: {:.4f}".format(step, loss_fn(logistic_regression(features),labels)))

def accuracy_fn(hypothesis, labels):                      #새로운 데이터를 통해 정확성 확인
    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)               #가설을 통한 예측값
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))       #실제값과 예측값 비교
    return accuracy                   #정확도 
        
test_acc = accuracy_fn(logistic_regression(x_test),y_test)      #가설(logistic regression)에 대한 결과 출력
